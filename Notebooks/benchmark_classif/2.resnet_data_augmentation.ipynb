{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <u>**Resnet 50 avec augmentation de données** </u></center>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 12:56:13.867312: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-03 12:56:13.896256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main folder\n",
    "path_to_folder = '../data_bees_detection/dataset_classification'\n",
    "\n",
    "classes = os.listdir(os.path.join(path_to_folder, 'train'))\n",
    "nb_classes = len(classes)  \n",
    "\n",
    "# function to load a split of the dataset\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    target_files = np.array(data['filenames'])\n",
    "    target_labels = keras.utils.to_categorical(np.array(data['target']), nb_classes)\n",
    "    return target_files, target_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "train_files, train_labels = load_dataset(os.path.join(path_to_folder, 'train'))\n",
    "valid_files, valid_labels = load_dataset(os.path.join(path_to_folder, 'validation'))\n",
    "test_files, test_labels = load_dataset(os.path.join(path_to_folder, 'test'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de l'augmentation et de la normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function to augment the images\n",
    "import cv2 \n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations import (Compose, Rotate, HorizontalFlip, VerticalFlip, Affine, RandomBrightnessContrast, ChannelShuffle)\n",
    "import albumentations as A\n",
    "\n",
    "AUGMENTATIONS = Compose([\n",
    "    Rotate(limit=[0,100], p=0.5),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    Affine(shear=[-45, 45], p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet import preprocess_input\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return preprocess_input(np.expand_dims(x, axis=0))\n",
    "\n",
    "class BatchGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, paths, labels, batch_size, augmentations,normalize=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augmentations\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        # nb of batches per epoch\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def load_img(self, img_path):\n",
    "\n",
    "        # loads img \n",
    "        img = load_img(img_path, target_size=(224,224))\n",
    "        x = img_to_array(img)\n",
    "        \n",
    "        # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "        x =  np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        return x\n",
    "\n",
    "    def apply_augmentation(self, batch_imgs):\n",
    "        # apply augmentation to a batch of images\n",
    "\n",
    "        batch_imgs = batch_imgs.astype(np.uint8)\n",
    "        \n",
    "\n",
    "        for i in range(len(batch_imgs)):\n",
    "            batch_imgs[i] = self.augment(image=batch_imgs[i])['image']\n",
    "\n",
    "        return batch_imgs\n",
    "    \n",
    "    def normalize_batch(self, batch_imgs):\n",
    "        # normalize a batch of images\n",
    "\n",
    "        batch_imgs = batch_imgs.astype(np.float32)\n",
    "\n",
    "        # normalize with imagenet stats\n",
    "        mean = [125.3, 123.0, 113.9]\n",
    "        std  = [63.0,  62.1,  66.7]\n",
    "\n",
    "        for i in range(len(batch_imgs)):\n",
    "            batch_imgs[i] -= mean\n",
    "            batch_imgs[i] /= std\n",
    "\n",
    "        return batch_imgs\n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get batch at position index\n",
    "\n",
    "        batch_paths = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_imgs = np.zeros((len(batch_paths), 224, 224, 3))\n",
    "        for i, f in enumerate(batch_paths):\n",
    "            batch_imgs[i] = self.load_img(f)\n",
    "\n",
    "        if self.augment: \n",
    "            batch_imgs = self.apply_augmentation(batch_imgs)\n",
    "\n",
    "        if self.normalize:\n",
    "            batch_imgs = self.normalize_batch(batch_imgs)\n",
    "            \n",
    "        return batch_imgs, batch_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = BatchGenerator(train_files, train_labels, 16, AUGMENTATIONS,True)\n",
    "ds_valid = BatchGenerator(valid_files, valid_labels, 16,  AUGMENTATIONS,True)\n",
    "ds_test = BatchGenerator(test_files, test_labels, 16,  AUGMENTATIONS,True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choix de l'architecture du modèle, des paramètres et hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 12:56:18.186791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.191695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.191850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.192891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.193064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.193146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.498486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.498697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.498816: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 12:56:18.498903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2594 MB memory:  -> device: 0, name: NVIDIA T600 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "# we take resnet50 as convolutional base with weights trained on imagenet\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# we add the classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "# we create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opti = Adam(learning_rate=0.0001)\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "model.compile(optimizer=opti, loss='categorical_crossentropy', metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# checkpoint to save the best model\n",
    "path_to_weights = '/datafiles/classification/saved_weights/benchmark_weights/2_resnet_augmentation.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=path_to_weights, monitor='val_categorical_accuracy', save_best_only=True, save_weights_only=False)\n",
    "\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=10)\n",
    "\n",
    "# reduce learning rate\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.1, patience=5, min_lr=0.00001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(ds_train, epochs=100, validation_data=ds_valid, callbacks=[checkpoint, early_stopping, reduce_lr])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24066/3983120818.py:11: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  model.evaluate_generator(ds_test)\n",
      "2023-07-03 13:04:40.207370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2382.351806640625,\n",
       " 0.010671130381524563,\n",
       " 0.010671130381524563,\n",
       " 0.010671130381524563]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "path_test = '../data_bees_detection/dataset_classification/test'\n",
    "\n",
    "test_files, test_labels = load_dataset(path_test)\n",
    "\n",
    "ds_test = BatchGenerator(test_files, test_labels, 16, None)\n",
    "\n",
    "# load the best model\n",
    "model.load_weights('/datafiles/classification/saved_weights/benchmark_weights/2_resnet_augmentation.h5')\n",
    "\n",
    "# evaluate the model\n",
    "model.evaluate_generator(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_bees_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
